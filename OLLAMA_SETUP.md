# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Qwen 2.5 7B –∑ Ollama

## –°–∏—Å—Ç–µ–º–Ω—ñ –≤–∏–º–æ–≥–∏ ‚úÖ

**–í–∞—à–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –ø—ñ–¥—Ö–æ–¥–∏—Ç—å —ñ–¥–µ–∞–ª—å–Ω–æ!**

- ‚úÖ RTX 3070 (8GB VRAM)
- ‚úÖ 24GB RAM
- ‚úÖ Windows 10/11

### –©–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏:

1. **Docker Desktop** –∑ WSL2
2. **NVIDIA Container Toolkit** –¥–ª—è GPU –ø—ñ–¥—Ç—Ä–∏–º–∫–∏
3. **CUDA –¥—Ä–∞–π–≤–µ—Ä–∏** (–≤–µ—Ä—Å—ñ—è 11.8+)

---

## ‚ö° –ù–∞–π—à–≤–∏–¥—à–∏–π —Å—Ç–∞—Ä—Ç (Lazy Loading)

```bash
# 1. –ó–∞–ø—É—Å—Ç–∏—Ç–∏ (—Å—Ç–∞—Ä—Ç –∑–∞ ~5 —Å–µ–∫—É–Ω–¥!)
docker-compose up -d

# 2. –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∏ –ü–ï–†–®–û–ú–£ –∑–∞–ø–∏—Ç—ñ
# –ü–µ—Ä—à–∏–π —Ä–∞–∑: ~10 —Ö–≤ (–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ)
# –í—Å—ñ –Ω–∞—Å—Ç—É–ø–Ω—ñ —Ä–∞–∑–∏: –º–∏—Ç—Ç—î–≤–æ ‚ö°
```

**–Ø–∫ —Ü–µ –ø—Ä–∞—Ü—é—î:**
- Ollama —Å–µ—Ä–≤–µ—Ä —Å—Ç–∞—Ä—Ç—É—î **–º–∏—Ç—Ç—î–≤–æ**
- –ü—Ä–∏ –ø–µ—Ä—à–æ–º—É –≤–∏–∫–ª–∏–∫—É –∞–≥–µ–Ω—Ç–∞ ‚Üí –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
- Volume –∑–±–µ—Ä—ñ–≥–∞—î –º–æ–¥–µ–ª—å ‚Üí –Ω–∞—Å—Ç—É–ø–Ω—ñ –∑–∞–ø—É—Å–∫–∏ –º–∏—Ç—Ç—î–≤—ñ

---

## –ü–æ–≤–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è üöÄ

### 1. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è NVIDIA Container Toolkit

```powershell
# –£ PowerShell –∑ –ø—Ä–∞–≤–∞–º–∏ –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
wsl --install
wsl --set-default-version 2
```

–í WSL —Ç–µ—Ä–º—ñ–Ω–∞–ª—ñ:
```bash
# –î–æ–¥–∞—Ç–∏ NVIDIA repository
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ nvidia-container-toolkit
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ GPU –≤ Docker

```bash
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

–ü–æ–≤–∏–Ω–µ–Ω –ø–æ–∫–∞–∑–∞—Ç–∏ –≤–∞—à—É RTX 3070!

### 3. –ó–∞–ø—É—Å–∫ Ollama –∑ Qwen 2.5 7B

```bash
# –ó–±—ñ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ñ–≤
docker-compose build

# –ó–∞–ø—É—Å–∫ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç—å –º–æ–¥–µ–ª—å ~4.7GB)
docker-compose up -d

# –ü–µ—Ä–µ–≥–ª—è–¥ –ª–æ–≥—ñ–≤ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
docker-compose logs -f ollama
```

–ü–µ—Ä—à–∏–π –∑–∞–ø—É—Å–∫ –∑–∞–π–º–µ **5-10 —Ö–≤–∏–ª–∏–Ω** (–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ).

### 4. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–±–æ—Ç–∏

```bash
# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —â–æ Ollama –ø—Ä–∞—Ü—é—î
curl http://localhost:11434/api/tags

# –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç –¥–æ –º–æ–¥–µ–ª—ñ
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b-instruct-q5_K_M",
  "prompt": "–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?"
}'
```

---

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ –±–æ—Ç—ñ ü§ñ

### –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥:

```python
from bot.services.ollama_service import get_ollama_service

# –û—Ç—Ä–∏–º–∞—Ç–∏ —Å–µ—Ä–≤—ñ—Å
service = get_ollama_service()

# –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
response = await service.generate_response(
    prompt="–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É",
    system_prompt="–¢–∏ Python –µ–∫—Å–ø–µ—Ä—Ç",
    temperature=0.5
)

print(response)
```

### –ó —Å—Ç—Ä—ñ–º—ñ–Ω–≥–æ–º:

```python
async for chunk in service.generate_response_stream(
    prompt="–†–æ–∑–∫–∞–∂–∏ –ø—Ä–æ –º—É–ª—å—Ç—ñ-–∞–≥–µ–Ω—Ç–Ω—ñ —Å–∏—Å—Ç–µ–º–∏"
):
    print(chunk, end="", flush=True)
```

### –ó –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ä–æ–∑–º–æ–≤–∏:

```python
conversation = [
    {"role": "user", "content": "–ü—Ä–∏–≤—ñ—Ç!"},
    {"role": "assistant", "content": "–ü—Ä–∏–≤—ñ—Ç! –ß–∏–º –º–æ–∂—É –¥–æ–ø–æ–º–æ–≥—Ç–∏?"},
    {"role": "user", "content": "–©–æ —Ç–∞–∫–µ Docker?"}
]

response = await service.generate_with_context(conversation)
```

**–ë—ñ–ª—å—à–µ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤:** `examples/ollama_usage_example.py`

---

## –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –ü–ö üíª

### –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –≤ `.env`:

```env
# –î–ª—è –ø–æ—Ç—É–∂–Ω–∏—Ö –ü–ö (RTX 3070+, 16GB+ RAM)
OLLAMA_MODEL=qwen2.5:7b-instruct-q5_K_M
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_CPU_LIMIT=4
OLLAMA_MEMORY_LIMIT=8G

# –î–ª—è —Å–µ—Ä–µ–¥–Ω—ñ—Ö –ü–ö (GTX 1660, 8-16GB RAM)
OLLAMA_MODEL=qwen2.5:7b-instruct-q4_K_M
OLLAMA_NUM_PARALLEL=1
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_CPU_LIMIT=2
OLLAMA_MEMORY_LIMIT=6G

# –î–ª—è —Å–ª–∞–±–∫–∏—Ö –ü–ö (–±–µ–∑ GPU, 8GB RAM)
OLLAMA_MODEL=qwen2.5:3b-instruct-q4_K_M
OLLAMA_NUM_PARALLEL=1
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_CPU_LIMIT=2
OLLAMA_MEMORY_LIMIT=4G
```

### –ü–æ—è—Å–Ω–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤:

- **OLLAMA_NUM_PARALLEL** - –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ (1-4)
- **OLLAMA_MAX_LOADED_MODELS** - –º–∞–∫—Å–∏–º—É–º –º–æ–¥–µ–ª–µ–π –≤ RAM
- **OLLAMA_CPU_LIMIT** - –ª—ñ–º—ñ—Ç CPU cores
- **OLLAMA_MEMORY_LIMIT** - –º–∞–∫—Å–∏–º—É–º RAM –¥–ª—è Ollama

### ‚ö†Ô∏è –í–ê–ñ–õ–ò–í–û: –ü—ñ—Å–ª—è –∑–º—ñ–Ω–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó

```bash
# –ó—É–ø–∏–Ω–∏—Ç–∏ —ñ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏ –∑ –Ω–æ–≤–∏–º–∏ env –∑–º—ñ–Ω–Ω–∏–º–∏
docker-compose down
docker-compose up -d --force-recreate

# –ê–ë–û –∫–æ—Ä–æ—Ç—à–µ (–ø–µ—Ä–µ—Å–æ–∑–¥–∞—î –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
docker-compose up -d --force-recreate

# –Ø–∫—â–æ –∑–º—ñ–Ω—é–≤–∞–ª–∏ Dockerfile (—Ä—ñ–¥–∫–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
docker-compose up -d --build --force-recreate
```

**–ü–æ—è—Å–Ω–µ–Ω–Ω—è:**
- `docker-compose up --build` - –ø–µ—Ä–µ–±—É–¥–æ–≤—É—î image, –∞–ª–µ **–ù–ï** –ø—ñ–¥—Ö–æ–ø–ª—é—î –Ω–æ–≤—ñ env –∑–º—ñ–Ω–Ω—ñ
- `--force-recreate` - **–û–ë–û–í'–Ø–ó–ö–û–í–û** –¥–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∑–º—ñ–Ω –∑ `.env`
- –ú–æ–¥–µ–ª—å –ù–ï –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è –∑–∞–Ω–æ–≤–æ (–∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –≤ volume `ollama_data`)

---

## –ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –Ω–∞ RTX 3070 ‚ö°

| –ú–æ–¥–µ–ª—å | VRAM | RAM | –®–≤–∏–¥–∫—ñ—Å—Ç—å | –Ø–∫—ñ—Å—Ç—å |
|--------|------|-----|-----------|--------|
| **qwen2.5:7b-instruct-q5_K_M** ‚≠ê | 5.5 GB | 6 GB | ~40-50 tok/s | –í—ñ–¥–º—ñ–Ω–Ω–∞ |
| qwen2.5:7b-instruct-q4_K_M | 4.5 GB | 5 GB | ~50-60 tok/s | –î—É–∂–µ –¥–æ–±—Ä–∞ |
| qwen2.5:7b-instruct-q8_0 | 7 GB | 8 GB | ~30-40 tok/s | –ù–∞–π–∫—Ä–∞—â–∞ |
| qwen2.5:3b-instruct-q4_K_M | 2 GB | 3 GB | ~70-80 tok/s | –î–æ–±—Ä–∞ |

**–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞:** `q5_K_M` - –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –±–∞–ª–∞–Ω—Å!

---

## –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

### –ó–º—ñ–Ω–∏—Ç–∏ –º–æ–¥–µ–ª—å:

–í—ñ–¥—Ä–µ–¥–∞–≥—É–π—Ç–µ `docker-compose.yml`:

```yaml
command: >
  sh -c "ollama serve &
         sleep 5 &&
         ollama pull qwen2.5:7b-instruct-q4_K_M &&  # –ó–º—ñ–Ω–∏—Ç–∏ —Ç—É—Ç
         wait"
```

### –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–µ—Ä–≤—ñ—Å—É:

–£ `bot/services/ollama_service.py`:

```python
ollama_service = OllamaService(
    host="http://ollama:11434",
    model="qwen2.5:7b-instruct-q5_K_M"  # –ó–º—ñ–Ω–∏—Ç–∏ –º–æ–¥–µ–ª—å
)
```

---

## –ö–æ—Ä–∏—Å–Ω—ñ –∫–æ–º–∞–Ω–¥–∏ üõ†Ô∏è

```bash
# –ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –ª–æ–≥–∏
docker-compose logs -f ollama

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–∏ —Å–µ—Ä–≤—ñ—Å
docker-compose restart ollama

# –ó—É–ø–∏–Ω–∏—Ç–∏ –≤—Å–µ
docker-compose down

# –í–∏–¥–∞–ª–∏—Ç–∏ —ñ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å (–∑ –≤–∏–¥–∞–ª–µ–Ω–Ω—è–º –º–æ–¥–µ–ª—ñ)
docker-compose down -v
docker-compose up -d

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU
docker exec ollama-qwen nvidia-smi

# –°–ø–∏—Å–æ–∫ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
docker exec ollama-qwen ollama list

# –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞ –∫–æ–Ω—Å–æ–ª—å
docker exec -it ollama-qwen ollama run qwen2.5:7b-instruct-q5_K_M
```

---

## –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è –º—É–ª—å—Ç—ñ-–∞–≥–µ–Ω—Ç—ñ–≤ üéØ

### –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:

```python
# –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä (Qwen)
coordinator = OllamaService(model="qwen2.5:7b-instruct-q5_K_M")

# –ê–≥–µ–Ω—Ç–∏ –º–æ–∂—É—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ç–æ–π —Å–∞–º–∏–π —Å–µ—Ä–≤—ñ—Å
class CodeAgent:
    def __init__(self):
        self.llm = get_ollama_service()
        self.system_prompt = "–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è"

class TestAgent:
    def __init__(self):
        self.llm = get_ollama_service()
        self.system_prompt = "–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"
```

### Frameworks –¥–ª—è –º—É–ª—å—Ç—ñ-–∞–≥–µ–Ω—Ç—ñ–≤:

1. **AutoGen** (Microsoft)
   ```bash
   pip install pyautogen
   ```

2. **LangGraph** (LangChain)
   ```bash
   pip install langgraph
   ```

3. **CrewAI**
   ```bash
   pip install crewai
   ```

---

## Troubleshooting üîß

### –ü–æ–º–∏–ª–∫–∞ "GPU not found"

```bash
# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ CUDA –¥—Ä–∞–π–≤–µ—Ä–∏
nvidia-smi

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–∏ Docker
sudo systemctl restart docker
```

### –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è –ø–æ–≤—ñ–ª—å–Ω–æ

–¶–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –∑–∞–ø—É—Å–∫—É (~4.7GB). –ù–∞—Å—Ç—É–ø–Ω—ñ –∑–∞–ø—É—Å–∫–∏ –º–∏—Ç—Ç—î–≤—ñ.

### Out of Memory

–í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ –º–µ–Ω—à—É –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—é:
- q5_K_M ‚Üí q4_K_M (–µ–∫–æ–Ω–æ–º—ñ—è ~1GB)

### Ollama —Å–µ—Ä–≤—ñ—Å –Ω–µ —Å—Ç–∞—Ä—Ç—É—î

```bash
# –ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—ñ –ª–æ–≥–∏
docker-compose logs ollama

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –ø–æ—Ä—Ç
netstat -an | grep 11434
```

---

## –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ñ –º–æ–¥–µ–ª—ñ üîÑ

–ú–æ–∂–Ω–∞ –ª–µ–≥–∫–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–∏—Å—è –Ω–∞ —ñ–Ω—à—ñ –º–æ–¥–µ–ª—ñ:

```bash
# –£ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ñ
docker exec ollama-qwen ollama pull llama3.1:8b
docker exec ollama-qwen ollama pull mistral:7b
docker exec ollama-qwen ollama pull codellama:7b
```

–ü–æ—Ç—ñ–º –∑–º—ñ–Ω–∏—Ç–∏ –≤ –∫–æ–¥—ñ:
```python
service = OllamaService(model="llama3.1:8b")
```

---

## –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ üìä

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤:

```bash
# –†–µ–∞–ª—å–Ω–∏–π —á–∞—Å
docker stats ollama-qwen

# GPU –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
watch -n 1 nvidia-smi
```

### Benchmarking:

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥–∏
docker-compose run --rm bot python examples/ollama_usage_example.py
```

---

## –ü–∏—Ç–∞–Ω–Ω—è? üí¨

- [Ollama GitHub](https://github.com/ollama/ollama)
- [Qwen 2.5 Documentation](https://qwenlm.github.io/blog/qwen2.5/)
- [Hugging Face - Qwen2.5](https://huggingface.co/Qwen)

---

**–ì–æ—Ç–æ–≤–æ –¥–æ —Ä–æ–±–æ—Ç–∏!** üéâ

–ó–∞–ø—É—Å–∫–∞–π: `docker-compose up -d`
